{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#                      Project : Predicting Stock Price\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# Clear Screen\n",
    "# \n",
    "print(chr(27) + \"[2J\")\n",
    "\n",
    "\n",
    "# Banner\n",
    "#\n",
    "print('')\n",
    "print('')\n",
    "print('           # #                 # #                 # #      ')\n",
    "print('           # #                 # #                 # #      ')\n",
    "print('          $$$$$               $$$$$               $$$$$     ')\n",
    "print('        $$ # # $$           $$ # # $$           $$ # # $$   ')\n",
    "print('       $$$ # #             $$$ # #             $$$ # #      ')\n",
    "print('        $$$# #              $$$# #              $$$# #      ')\n",
    "print('          $$$#                $$$#                $$$#      ')\n",
    "print('           #$$$                #$$$                #$$$     ')\n",
    "print('           # #$$$              # #$$$              # #$$$   ')\n",
    "print('           # # $$$             # # $$$             # # $$$  ')\n",
    "print('       $$$ # #  $$$$       $$$ # #  $$$$       $$$ # #  $$$$')\n",
    "print('        $$$# # $$$$         $$$# # $$$$         $$$# # $$$$ ')\n",
    "print('          $$$$$$$             $$$$$$$             $$$$$$$   ')\n",
    "print('           # #                 # #                 # #      ')\n",
    "print('           # #                 # #                 # #      ')\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ccf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Enabling Logging\n",
    "# ==============================================================================\n",
    "\n",
    "import logging\n",
    "\n",
    "# Logger Settings:\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "def setup_logger(name, log_file, level=logging.INFO):\n",
    "    \"\"\"To setup as many loggers as you want\"\"\"\n",
    "    handler = logging.FileHandler(log_file)        \n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "# Phases Logger\n",
    "phases_logger_file = 'log_phases.log'\n",
    "os.system('rm -rf '+ phases_logger_file)\n",
    "phases_logger = setup_logger('Phases', phases_logger_file)\n",
    "phases_logger.info('Project : Predicting Stock Price')\n",
    "phases_logger.info('Phases Logger Init')\n",
    "\n",
    "# Details Logger\n",
    "details_logger_file = 'log_details.log'\n",
    "os.system('rm -rf '+ details_logger_file)\n",
    "details_logger = setup_logger('Details', details_logger_file)\n",
    "details_logger.info('Project : Predicting Stock Price')\n",
    "details_logger.info('Details Logger Init')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ab6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Importing Basic Packages\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Importing Basic Packages : Start')\n",
    "\n",
    "details_logger.info('Import : re')\n",
    "import re\n",
    "\n",
    "details_logger.info('Import : os')\n",
    "import os\n",
    "\n",
    "details_logger.info('Import : sys')\n",
    "import sys\n",
    "\n",
    "phases_logger.info('Importing Basic Packages : End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Importing Project Specific Packages\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Importing Project Specific Packages : Start')\n",
    "\n",
    "# Identifying Customer Targets (Python)\n",
    "details_logger.info('Import : inline')\n",
    "%matplotlib inline\n",
    "\n",
    "# import packages for text processing and machine learning\n",
    "details_logger.info('Import : pandas')\n",
    "import pandas as pd  # DataFrame structure and operations\n",
    "from pandas.plotting import scatter_matrix  # scatter plot matrix\n",
    "\n",
    "details_logger.info('Import : numpy')\n",
    "import numpy as np  # arrays and numerical processing\n",
    "import matplotlib.pyplot as plt  # 2D plotting\n",
    "\n",
    "details_logger.info('Import : scipy')\n",
    "from scipy.stats import uniform  # for training-and-test split\n",
    "\n",
    "details_logger.info('Import : patsy')\n",
    "import patsy  # translate model specification into design matrices\n",
    "\n",
    "details_logger.info('Import : seaborn')\n",
    "import seaborn as sns  # PROVIDES TRELLIS AND SMALL MULTIPLE PLOTTING\n",
    "\n",
    "# import user-defined module\n",
    "# details_logger.info('import evaluate_classifier')\n",
    "# import evaluate_classifier as eval\n",
    "\n",
    "# FOLLOWING PACKAGE BEST IMPORTED AND INSTALLED VIA CONDA PROMPT\n",
    "# conda install -c conda-forge mlxtend\n",
    "\n",
    "# Association Rules Mining\n",
    "# details_logger.info('import mlxtend')\n",
    "# from mlxtend.frequent_patterns import apriori            # EASY ASSOCIATION RULES PACKAGE FROM RABST\n",
    "# from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "details_logger.info('Import : queue')\n",
    "from queue import Queue\n",
    "\n",
    "details_logger.info('Import : threading')\n",
    "import threading\n",
    "\n",
    "details_logger.info('Import : time')\n",
    "import time\n",
    "\n",
    "details_logger.info('Import : sklearn')\n",
    "from sklearn.tree import DecisionTreeRegressor  # machine learning tree\n",
    "from sklearn.ensemble import RandomForestRegressor # ensemble method\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "details_logger.info('Import : statsmodels')\n",
    "import statsmodels.api as sm  # logistic regression\n",
    "import statsmodels.formula.api as smf  # R-like model specification\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "details_logger.info('Import : pmdarima')\n",
    "import pmdarima as pm\n",
    "\n",
    "phases_logger.info('Project Specific Package Load : End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Present Working Directory\n",
    "# ==============================================================================\n",
    "\n",
    "# This will print the current directory : Debugging purposes\n",
    "details_logger.info('PWD: ' + os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Information\n",
    "# ==============================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Following should be your tree such that the link 'stock_market_data' points to\n",
    "# actual directory 'stock_market_data' that is 2 directories above\n",
    "\n",
    "# MY_PROJECT_DIR\n",
    "# ├── BITBUCKET_CHECKOUT_DIR_000\n",
    "# │   └── predictingstockprice\n",
    "# │       ├── OtherTSVs\n",
    "# │       ├── lib\n",
    "# │       └── stock_market_data -> ../../stock_market_data\n",
    "# └── stock_market_data\n",
    "#     ├── forbes2000\n",
    "#     │   ├── csv\n",
    "#     │   └── json\n",
    "#     ├── nasdaq\n",
    "#     │   ├── csv\n",
    "#     │   └── json\n",
    "#     ├── nyse\n",
    "#     │   ├── csv\n",
    "#     │   └── json\n",
    "#     └── sp500\n",
    "#         ├── csv\n",
    "#         └── json\n",
    "\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# User Settings\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('User Settings : Start')\n",
    "\n",
    "\n",
    "# Using stock_market_data link in the current directory\n",
    "# stock_market_data -> ../../stock_market_data\n",
    "#\n",
    "stock_market_data_path = os.path.realpath('stock_market_data')\n",
    "\n",
    "\n",
    "## Uncomment following to override the stock_market_data_path\n",
    "\n",
    "# USER_SETTING_HERE\n",
    "# \n",
    "# If the tree structure thing does not work for you, you may have to use the following paths instead:\n",
    "## Ayush:\n",
    "# stock_market_data_path = 'Add_your_path_here'\n",
    "## CB:\n",
    "# stock_market_data_path = '/Users/cbgarrett/Documents/ist718project/stock_market_data'\n",
    "## Richard:\n",
    "# stock_market_data_path = 'Add_your_path_here'\n",
    "## Niranjan:\n",
    "# stock_market_data_path = '/Users/niranjanjuvekar/MyStuff/Education_Niranjan/Data_Science_Syracuse_University/12_IST_718/Project/stock_market_data'\n",
    "\n",
    "\n",
    "# Logging User Settings:\n",
    "#\n",
    "details_logger.info('User Settings:')\n",
    "details_logger.info('User Setting : stock_market_data_path = ' + stock_market_data_path)\n",
    "\n",
    "# USER_SETTING_HERE\n",
    "# \n",
    "workOnMiniDataFrame = True\n",
    "\n",
    "\n",
    "if workOnMiniDataFrame:\n",
    "    construct_Low_df           = False\n",
    "    construct_Open_df          = True\n",
    "    construct_Volume_df        = False\n",
    "    construct_High_df          = False\n",
    "    construct_Close_df         = False\n",
    "    construct_AdjustedClose_df = False\n",
    "else:\n",
    "    # USER_SETTING_HERE\n",
    "    # \n",
    "    construct_Low_df           = False\n",
    "    construct_Open_df          = True\n",
    "    construct_Volume_df        = False\n",
    "    construct_High_df          = False\n",
    "    construct_Close_df         = False\n",
    "    construct_AdjustedClose_df = False\n",
    "\n",
    "\n",
    "if construct_Low_df:\n",
    "    details_logger.info('User Setting : construct_Low_df = True')\n",
    "else:\n",
    "    details_logger.info('User Setting : construct_Low_df = False')\n",
    "\n",
    "if construct_Open_df:\n",
    "    details_logger.info('User Setting : construct_Open_df = True')\n",
    "else:\n",
    "    details_logger.info('User Setting : construct_Open_df = False')\n",
    "\n",
    "if construct_Volume_df:\n",
    "    details_logger.info('User Setting : construct_Volume_df = True')\n",
    "else:\n",
    "    details_logger.info('User Setting : construct_Volume_df = False')\n",
    "\n",
    "if construct_High_df:\n",
    "    details_logger.info('User Setting : construct_High_df = True')\n",
    "else:\n",
    "    details_logger.info('User Setting : construct_High_df = False')\n",
    "\n",
    "if construct_Close_df:\n",
    "    details_logger.info('User Setting : construct_Close_df = True')\n",
    "else:\n",
    "    details_logger.info('User Setting : construct_Close_df = False')\n",
    "\n",
    "if construct_AdjustedClose_df:\n",
    "    details_logger.info('User Setting : construct_AdjustedClose_df = True')\n",
    "else:\n",
    "    details_logger.info('User Setting : construct_AdjustedClose_df = False')\n",
    "\n",
    "\n",
    "# Variable breakAtIteration to be used for debugging purposes\n",
    "# if breakAtIteration = 0 : goes through all files\n",
    "#\n",
    "# USER_SETTING_HERE\n",
    "#\n",
    "if workOnMiniDataFrame:\n",
    "    breakAtIteration = 500\n",
    "else:\n",
    "    breakAtIteration = 0\n",
    "\n",
    "\n",
    "details_logger.info('User Setting : breakAtIteration = ' + str(breakAtIteration))\n",
    "\n",
    "\n",
    "# Once the dataFrame has been processed, we can\n",
    "# write it to the disk in TSV or CSV format\n",
    "#\n",
    "# USER_SETTING_HERE\n",
    "#\n",
    "write_CSV = False\n",
    "write_TSV = False\n",
    "\n",
    "\n",
    "details_logger.info('User Setting : write_CSV = ' + str(write_CSV))\n",
    "details_logger.info('User Setting : write_TSV = ' + str(write_TSV))\n",
    "\n",
    "\n",
    "# USER_SETTINGS_HERE\n",
    "# \n",
    "# maxThreads = 2\n",
    "maxThreads = 6\n",
    "\n",
    "\n",
    "# Number of models to be run\n",
    "N = 1 # Run only one model (Default)\n",
    "if workOnMiniDataFrame:\n",
    "    # N = 4\n",
    "    N = 500\n",
    "else:\n",
    "    # USER_SETTING_HERE\n",
    "    # \n",
    "    # N = df.shape[1]-2\n",
    "    # N = 10000\n",
    "    # N = 1000\n",
    "    # N = 1500\n",
    "    # N = 100\n",
    "    # N = 10\n",
    "    pass\n",
    "\n",
    "    \n",
    "ARIMA_models_dict1 = {}\n",
    "ARIMA_forecast_dict1 = {}\n",
    "ARIMA_model_residuals_dict1 = {}\n",
    "    \n",
    "phases_logger.info('User Settings : End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd01a364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' GOOD CODE\n",
    "\n",
    "# ==============================================================================\n",
    "# DataFrame Construction\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Data Frames Construction : Start')\n",
    "\n",
    "\n",
    "# Initializing variables in needed for constructing the dataFrames\n",
    "# Dictionary of all CSVs is CSV_dict\n",
    "#\n",
    "CSV_dict = {}\n",
    "iterationNumber=0\n",
    "breakOuterLoop = False\n",
    "iter1TickerSymbol = ''\n",
    "\n",
    "\n",
    "# Date Column of all the dataFrames\n",
    "#\n",
    "if construct_Low_df:\n",
    "    Low_df           = pd.read_csv('OtherTSVs/AllDates.csv', sep=',', header='infer')\n",
    "if construct_Open_df:\n",
    "    Open_df          = pd.read_csv('OtherTSVs/AllDates.csv', sep=',', header='infer')\n",
    "if construct_Volume_df:\n",
    "    Volume_df        = pd.read_csv('OtherTSVs/AllDates.csv', sep=',', header='infer')\n",
    "if construct_High_df:\n",
    "    High_df          = pd.read_csv('OtherTSVs/AllDates.csv', sep=',', header='infer')\n",
    "if construct_Close_df:\n",
    "    Close_df         = pd.read_csv('OtherTSVs/AllDates.csv', sep=',', header='infer')\n",
    "if construct_AdjustedClose_df:\n",
    "    AdjustedClose_df = pd.read_csv('OtherTSVs/AllDates.csv', sep=',', header='infer')\n",
    "\n",
    "    \n",
    "# Find all the CSV files from the stock_market_data directory\n",
    "#\n",
    "for root, dirs, files in os.walk(stock_market_data_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            details_logger.info('ConstructDF: Root = ' + root)\n",
    "            details_logger.info('ConstructDF: File = ' + file)\n",
    "            details_logger.info('ConstructDF: FullFilePath = ' + os.path.join(root, file))\n",
    "            tickerSymbol = re.sub('.csv','',file)\n",
    "            details_logger.info('ConstructDF: tickerSymbol = ' + tickerSymbol)\n",
    "\n",
    "            \n",
    "            # Process only those files whose corresponding\n",
    "            # ticker symbols are not already processed\n",
    "            # \n",
    "            if tickerSymbol not in CSV_dict.keys():\n",
    "                iterationNumber += 1\n",
    "                details_logger.info('ConstructDF: iterationNumber = ' + str(iterationNumber) + ' : tickerSymbol = ' + tickerSymbol)\n",
    "                CSV_dict[tickerSymbol] = pd.read_csv(os.path.join(root, file), sep=',', header='infer')\n",
    "                \n",
    "                \n",
    "                # Process the datatype of the columns here (If needed)\n",
    "                #\n",
    "                \n",
    "                \n",
    "                # Merging the dataFrames\n",
    "                #\n",
    "                if construct_Low_df:\n",
    "                    Low_df = pd.merge(Low_df, CSV_dict[tickerSymbol][['Date', 'Low']],  how='left', left_on=['Date'], right_on = ['Date'], suffixes=('', '_'+tickerSymbol))\n",
    "                if construct_Open_df:\n",
    "                    Open_df = pd.merge(Open_df, CSV_dict[tickerSymbol][['Date', 'Open']],  how='left', left_on=['Date'], right_on = ['Date'], suffixes=('', '_'+tickerSymbol))\n",
    "                if construct_Volume_df:\n",
    "                    Volume_df = pd.merge(Volume_df, CSV_dict[tickerSymbol][['Date', 'Volume']],  how='left', left_on=['Date'], right_on = ['Date'], suffixes=('', '_'+tickerSymbol))\n",
    "                if construct_High_df:\n",
    "                    High_df = pd.merge(High_df, CSV_dict[tickerSymbol][['Date', 'High']],  how='left', left_on=['Date'], right_on = ['Date'], suffixes=('', '_'+tickerSymbol))\n",
    "                if construct_Close_df:\n",
    "                    Close_df = pd.merge(Close_df, CSV_dict[tickerSymbol][['Date', 'Close']],  how='left', left_on=['Date'], right_on = ['Date'], suffixes=('', '_'+tickerSymbol))\n",
    "                if construct_AdjustedClose_df:\n",
    "                    AdjustedClose_df = pd.merge(AdjustedClose_df, CSV_dict[tickerSymbol][['Date', 'Adjusted Close']],  how='left', left_on=['Date'], right_on = ['Date'], suffixes=('', '_'+tickerSymbol))\n",
    "                \n",
    "                # Save the first tickerSymbol (for column name renaming later)\n",
    "                if iterationNumber == 1:\n",
    "                    iter1TickerSymbol = tickerSymbol                    \n",
    "                \n",
    "                # Testing Purposes : Break the iterations at given threshold\n",
    "                if breakAtIteration == iterationNumber:\n",
    "                    details_logger.info('ConstructDF: Breaking inner loop on iteration ' + str(iterationNumber))\n",
    "                    breakOuterLoop = True\n",
    "                    break\n",
    "                    \n",
    "    # If inner loop is broken, outer loop also needs to be broken\n",
    "    if breakOuterLoop:\n",
    "        details_logger.info('ConstructDF: Breaking outer loop')\n",
    "        break\n",
    "\n",
    "# Rename the columns\n",
    "details_logger.info('ConstructDF: Renaming dataFrame columns')\n",
    "if construct_Low_df:\n",
    "    Low_df.rename(columns=lambda x: re.sub('Low_','',x), inplace=True)\n",
    "    Low_df.rename(columns=lambda x: re.sub('Low',iter1TickerSymbol,x), inplace=True)\n",
    "\n",
    "if construct_Open_df:\n",
    "    Open_df.rename(columns=lambda x: re.sub('Open_','',x), inplace=True)\n",
    "    Open_df.rename(columns=lambda x: re.sub('Open',iter1TickerSymbol,x), inplace=True)\n",
    "\n",
    "if construct_Volume_df:\n",
    "    Volume_df.rename(columns=lambda x: re.sub('Volume_','',x), inplace=True)\n",
    "    Volume_df.rename(columns=lambda x: re.sub('Volume',iter1TickerSymbol,x), inplace=True)\n",
    "\n",
    "if construct_High_df:\n",
    "    High_df.rename(columns=lambda x: re.sub('High_','',x), inplace=True)\n",
    "    High_df.rename(columns=lambda x: re.sub('High',iter1TickerSymbol,x), inplace=True)\n",
    "\n",
    "if construct_Close_df:\n",
    "    Close_df.rename(columns=lambda x: re.sub('Close_','',x), inplace=True)\n",
    "    Close_df.rename(columns=lambda x: re.sub('Close',iter1TickerSymbol,x), inplace=True)\n",
    "\n",
    "if construct_AdjustedClose_df:\n",
    "    AdjustedClose_df.rename(columns=lambda x: re.sub('Adjusted Close_','',x), inplace=True)\n",
    "    AdjustedClose_df.rename(columns=lambda x: re.sub('Adjusted Close',iter1TickerSymbol,x), inplace=True)\n",
    "        \n",
    "details_logger.info('ConstructDF: Data Frames Construction Done')\n",
    "phases_logger.info('Data Frames Construction : End')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb30193",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' GOOD CODE\n",
    "\n",
    "# ==============================================================================\n",
    "# Sampling Constructed DataFrames\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Sampling Constructed DataFrames : Start')\n",
    "\n",
    "print('--------------------------------------------------')\n",
    "\n",
    "if construct_Low_df:\n",
    "    print(Low_df)\n",
    "    print('--------------------------------------------------')\n",
    "if construct_Open_df:\n",
    "    print(Open_df)\n",
    "    print('--------------------------------------------------')\n",
    "if construct_Volume_df:\n",
    "    print(Volume_df)\n",
    "    print('--------------------------------------------------')\n",
    "if construct_High_df:\n",
    "    print(High_df)\n",
    "    print('--------------------------------------------------')\n",
    "if construct_Close_df:\n",
    "    print(Close_df)\n",
    "    print('--------------------------------------------------')\n",
    "if construct_AdjustedClose_df:\n",
    "    print(AdjustedClose_df)\n",
    "    print('--------------------------------------------------')\n",
    "\n",
    "phases_logger.info('Sampling Constructed DataFrames : End')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863920d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' GOOD CODE\n",
    "# ==============================================================================\n",
    "# Writing Constructed DataFrames\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Writing Constructed DataFrames : Start')\n",
    "\n",
    "if construct_Low_df:\n",
    "    if write_TSV:\n",
    "        Low_df.to_csv('Low_df.tsv', sep='\\t', encoding='utf-8')\n",
    "    if write_CSV:\n",
    "        Low_df.to_csv('Low_df.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "if construct_Open_df:\n",
    "    if write_TSV:\n",
    "        Open_df.to_csv('Open_df.tsv', sep='\\t', encoding='utf-8')\n",
    "    if write_CSV:\n",
    "        Open_df.to_csv('Open_df.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "if construct_Volume_df:\n",
    "    if write_TSV:\n",
    "        Volume_df.to_csv('Volume_df.tsv', sep='\\t', encoding='utf-8')\n",
    "    if write_CSV:\n",
    "        Volume_df.to_csv('Volume_df.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "if construct_High_df:\n",
    "    if write_TSV:\n",
    "        High_df.to_csv('High_df.tsv', sep='\\t', encoding='utf-8')\n",
    "    if write_CSV:\n",
    "        High_df.to_csv('High_df.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "if construct_Close_df:\n",
    "    if write_TSV:\n",
    "            Close_df.to_csv('Close_df.tsv', sep='\\t', encoding='utf-8')\n",
    "    if write_CSV:\n",
    "        Close_df.to_csv('Close_df.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "if construct_AdjustedClose_df:\n",
    "    if write_TSV:\n",
    "        AdjustedClose_df.to_csv('AdjustedClose_df.tsv', sep='\\t', encoding='utf-8')\n",
    "    if write_CSV:\n",
    "        AdjustedClose_df.to_csv('AdjustedClose_df.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "phases_logger.info('Writing Constructed DataFrames : End')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7203db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SAMPLE CODE : UNCOMMENT WITH DISCRETION\n",
    "\n",
    "# ==============================================================================\n",
    "# Sample code to get smaller dataframe out of Open_df\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "phases_logger.info('Mini Data Frame Construction : Start')\n",
    "\n",
    "miniDF_tickerSymbols_Energy       = ['XOM', 'XEL', 'PLUG', 'FCEL']\n",
    "miniDF_tickerSymbols_Retail       = ['COST', 'MCD', 'SBUX', 'TJX', 'DLTR']\n",
    "miniDF_tickerSymbols_Banks        = ['BAC', 'V', 'WFC', 'JPM', 'MA', 'C', 'AXP']\n",
    "miniDF_tickerSymbols_HealthPharma = ['UNH', 'JNJ', 'LLY', 'ABBV', 'PFE', 'RHHBY', 'MRK', 'TMO', 'AZN', 'ABT', 'WBA', 'RIGL']\n",
    "miniDF_tickerSymbols_Technology   = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'NVDA', 'CSCO']\n",
    "miniDF_tickerSymbols_Automotive   = ['TM', 'TSLA', 'GM', 'F', 'AZO', 'HMC', 'NIO', 'CAR', 'PII', 'GT']\n",
    "miniDF_tickerSymbols_Cannabis     = ['INCR']\n",
    "\n",
    "\n",
    "miniDF_tickerSymbols = ['Date', 'WeekDay']\n",
    "\n",
    "\n",
    "miniDF_tickerSymbols.extend(miniDF_tickerSymbols_Energy)\n",
    "miniDF_tickerSymbols.extend(miniDF_tickerSymbols_Retail)\n",
    "miniDF_tickerSymbols.extend(miniDF_tickerSymbols_Banks)\n",
    "miniDF_tickerSymbols.extend(miniDF_tickerSymbols_HealthPharma)\n",
    "miniDF_tickerSymbols.extend(miniDF_tickerSymbols_Technology)\n",
    "miniDF_tickerSymbols.extend(miniDF_tickerSymbols_Automotive)\n",
    "miniDF_tickerSymbols.extend(miniDF_tickerSymbols_Cannabis)\n",
    "\n",
    "\n",
    "miniOpenDF_tickerSymbols = Open_df[miniDF_tickerSymbols]\n",
    "miniOpenDF_tickerSymbols\n",
    "                                     \n",
    "phases_logger.info('Mini Data Frame Construction : Start')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY HACK\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('Open_df.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a398d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "miniOpenDF_tickerSymbols = pd.read_csv('Open_df.csv', sep=',', header='infer')\n",
    "# miniOpenDF_tickerSymbols\n",
    "\n",
    "print(miniOpenDF_tickerSymbols.columns)\n",
    "\n",
    "# miniOpenDF_tickerSymbols = miniOpenDF_tickerSymbols[['CSCO', 'PLCE']]\n",
    "miniOpenDF_tickerSymbols['Date'] = miniOpenDF_tickerSymbols['Date'].str.replace('-','/')\n",
    "print(miniOpenDF_tickerSymbols)\n",
    "pd.to_datetime(miniOpenDF_tickerSymbols['Date'], infer_datetime_format=True)\n",
    "miniOpenDF_tickerSymbols = miniOpenDF_tickerSymbols.set_index('Date')\n",
    "\n",
    "# print(miniOpenDF_tickerSymbols)\n",
    "\n",
    "# Filling empty cells in between the filled cells\n",
    "miniOpenDF_tickerSymbols = miniOpenDF_tickerSymbols.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d76e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropNan (df):\n",
    "    ignoreColumns = ['Unnamed: 0', 'Date', 'WeekDay']\n",
    "    columns = df.columns\n",
    "    # print(columns)\n",
    "    for i in columns:\n",
    "        if i not in ignoreColumns:\n",
    "            # print('i = ', i)\n",
    "            df = df[df[i].notna()]\n",
    "    # print(df)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Example:\n",
    "mydf = dropNan(miniOpenDF_tickerSymbols[['CSCO', 'SBUX']])\n",
    "mydf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_logger.info('Initialize for Models Generation : Start')\n",
    "# Initialize for models\n",
    "\n",
    "# Threads Dictionary\n",
    "t = {}\n",
    "\n",
    "# Queue Settings\n",
    "q = Queue(maxsize = maxThreads)\n",
    "\n",
    "# Model Dictionary\n",
    "model = {}\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "\n",
    "phases_logger.info('Initialize for Models Generation : End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Function\n",
    "phases_logger.info('Create Model Functions : Start')\n",
    "\n",
    "def createARIMAModel1(modelNumber):\n",
    "    details_logger.info('Model number : ' + str(modelNumber))\n",
    "    print('AAA 1')\n",
    "    # columnNumber = miniOpenDF_tickerSymbols.columns[modelNumber+3]\n",
    "    columnName = miniOpenDF_tickerSymbols.columns[modelNumber+3]\n",
    "    # X_sel = (miniOpenDF_tickerSymbols[[columnName]])\n",
    "\n",
    "    X_sel = (miniOpenDF_tickerSymbols[[columnName]])\n",
    "    # X_sel = X_sel.set_index('Date')\n",
    "\n",
    "    print('AAA 4')\n",
    "    X_sel = X_sel.dropna()\n",
    "\n",
    "    print('AAA 2')\n",
    "\n",
    "    print(X_sel.shape)\n",
    "    print('AAA 3')\n",
    "\n",
    "    # X_sel = X_sel.set_index('Date')\n",
    "\n",
    "    print(X_sel)\n",
    "    \n",
    "    \n",
    "    X_sel = X_sel.replace(\"\", nan_value)\n",
    "\n",
    "\n",
    "    # print(np.asarray(X_sel))\n",
    "    print('AAA 5')\n",
    "\n",
    "    model[columnName] = ARIMA(X_sel.astype(float), order=(2, 0, 2))\n",
    "    # model[modelNumber] = ARIMA(X_sel[modelNumber], order=(1, 0, 0))\n",
    "    # print('Fitting model : ', modelNumber)\n",
    "    print('AAA 6')\n",
    "    ARIMA_models_dict1[columnName] = model[columnName].fit()\n",
    "\n",
    "    # Model Summary\n",
    "    # print(model_fit.summary())\n",
    "    print('AAA 7')\n",
    "\n",
    "    # plot residual errors\n",
    "    # ARIMA_model_residuals_dict1[columnName] = pd.DataFrame(ARIMA_models_dict1[columnName].resid)\n",
    "    # residuals.plot()\n",
    "    # plt.show()\n",
    "    # residuals.plot(kind='kde')\n",
    "    # plt.show()\n",
    "    # print(residuals.describe())\n",
    "    print('AAA 8')\n",
    "\n",
    "\n",
    "    # Create an array with forecast for the next days of the year\n",
    "    ARIMA_forecast_dict1[columnName] = ARIMA_models_dict1[columnName].forecast(steps=30)\n",
    "    print('AAA 9')\n",
    "    ARIMA_forecast_dict1[columnName] = pd.Series(ARIMA_forecast_dict1[columnName])\n",
    "    print('AAA A')\n",
    "\n",
    "    q.get()\n",
    "    print('AAA B')\n",
    "\n",
    "\n",
    "phases_logger.info('Create Model Functions : Start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_logger.info('Creating Threads : Start')\n",
    "\n",
    "for modelNumber in range(0,N,1):\n",
    "    details_logger.info('t[modelNumber] = ' + str(modelNumber))\n",
    "    t[modelNumber] = threading.Thread(target=createARIMAModel1, args=(modelNumber,))\n",
    "    \n",
    "phases_logger.info('Creating Threads : End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd92f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_logger.info('Making Models in Threads : Start')\n",
    "details_logger.info('Making Models in Threads')\n",
    "\n",
    "for modelNumber in range(0,N,1):\n",
    "    details_logger.info('Thread Spawn : ' + str(modelNumber) + ' : Starting')\n",
    "    while(q.full()):\n",
    "        print('Sleeping 1 sec')\n",
    "        details_logger.info('Sleeping 1 sec')\n",
    "        time.sleep(1)\n",
    "\n",
    "    details_logger.info('Q : put' + str(modelNumber))\n",
    "    q.put(modelNumber)\n",
    "    t[modelNumber].start()\n",
    "    details_logger.info('Thread Spawn : ' + str(modelNumber) + ' : Done')\n",
    "\n",
    "phases_logger.info('Making Models in Threads : End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e792cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_logger.info('Joining Threads : Start')\n",
    "\n",
    "for modelNumber in range(0,N,1):\n",
    "    t[modelNumber].join()\n",
    "    details_logger.info('Thread Join : ' + str(modelNumber))\n",
    "\n",
    "phases_logger.info('Joining Threads : End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Richie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193104d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355314f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
