{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2J\n",
      "\n",
      "\n",
      "           # #                 # #                 # #      \n",
      "           # #                 # #                 # #      \n",
      "          $$$$$               $$$$$               $$$$$     \n",
      "        $$ # # $$           $$ # # $$           $$ # # $$   \n",
      "       $$$ # #             $$$ # #             $$$ # #      \n",
      "        $$$# #              $$$# #              $$$# #      \n",
      "          $$$#                $$$#                $$$#      \n",
      "           #$$$                #$$$                #$$$     \n",
      "           # #$$$              # #$$$              # #$$$   \n",
      "           # # $$$             # # $$$             # # $$$  \n",
      "       $$$ # #  $$$$       $$$ # #  $$$$       $$$ # #  $$$$\n",
      "        $$$# # $$$$         $$$# # $$$$         $$$# # $$$$ \n",
      "          $$$$$$$             $$$$$$$             $$$$$$$   \n",
      "           # #                 # #                 # #      \n",
      "           # #                 # #                 # #      \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#                      Project : Predicting Stock Price\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# Clear Screen\n",
    "# \n",
    "print(chr(27) + \"[2J\")\n",
    "\n",
    "\n",
    "# Banner\n",
    "#\n",
    "print('')\n",
    "print('')\n",
    "print('           # #                 # #                 # #      ')\n",
    "print('           # #                 # #                 # #      ')\n",
    "print('          $$$$$               $$$$$               $$$$$     ')\n",
    "print('        $$ # # $$           $$ # # $$           $$ # # $$   ')\n",
    "print('       $$$ # #             $$$ # #             $$$ # #      ')\n",
    "print('        $$$# #              $$$# #              $$$# #      ')\n",
    "print('          $$$#                $$$#                $$$#      ')\n",
    "print('           #$$$                #$$$                #$$$     ')\n",
    "print('           # #$$$              # #$$$              # #$$$   ')\n",
    "print('           # # $$$             # # $$$             # # $$$  ')\n",
    "print('       $$$ # #  $$$$       $$$ # #  $$$$       $$$ # #  $$$$')\n",
    "print('        $$$# # $$$$         $$$# # $$$$         $$$# # $$$$ ')\n",
    "print('          $$$$$$$             $$$$$$$             $$$$$$$   ')\n",
    "print('           # #                 # #                 # #      ')\n",
    "print('           # #                 # #                 # #      ')\n",
    "print('')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Phases:Project : Predicting Stock Price\n",
      "INFO:Phases:Phases Logger Init\n",
      "INFO:Details:Project : Predicting Stock Price\n",
      "INFO:Details:Details Logger Init\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Enabling Logging\n",
    "# ==============================================================================\n",
    "\n",
    "import logging\n",
    "\n",
    "# Logger Settings:\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "def setup_logger(name, log_file, level=logging.INFO):\n",
    "    \"\"\"To setup as many loggers as you want\"\"\"\n",
    "    handler = logging.FileHandler(log_file)        \n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "# Phases Logger\n",
    "phases_logger_file = 'log_phases.log'\n",
    "os.system('rm -rf '+ phases_logger_file)\n",
    "phases_logger = setup_logger('Phases', phases_logger_file)\n",
    "phases_logger.info('Project : Predicting Stock Price')\n",
    "phases_logger.info('Phases Logger Init')\n",
    "\n",
    "# Details Logger\n",
    "details_logger_file = 'log_details.log'\n",
    "os.system('rm -rf '+ details_logger_file)\n",
    "details_logger = setup_logger('Details', details_logger_file)\n",
    "details_logger.info('Project : Predicting Stock Price')\n",
    "details_logger.info('Details Logger Init')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Phases:Importing Basic Packages : Start\n",
      "INFO:Details:Import : re\n",
      "INFO:Details:Import : os\n",
      "INFO:Details:Import : sys\n",
      "INFO:Phases:Importing Basic Packages : End\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Importing Basic Packages\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Importing Basic Packages : Start')\n",
    "\n",
    "details_logger.info('Import : re')\n",
    "import re\n",
    "\n",
    "details_logger.info('Import : os')\n",
    "import os\n",
    "\n",
    "details_logger.info('Import : sys')\n",
    "import sys\n",
    "\n",
    "phases_logger.info('Importing Basic Packages : End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Phases:Importing Project Specific Packages : Start\n",
      "INFO:Details:Import : inline\n",
      "INFO:Details:Import : pandas\n",
      "INFO:Details:Import : numpy\n",
      "INFO:Details:Import : scipy\n",
      "INFO:Details:Import : patsy\n",
      "INFO:Details:Import : seaborn\n",
      "INFO:Details:Import : queue\n",
      "INFO:Details:Import : threading\n",
      "INFO:Details:Import : time\n",
      "INFO:Details:Import : sklearn\n",
      "INFO:Details:Import : statsmodels\n",
      "INFO:Details:Import : pmdarima\n",
      "INFO:Details:Import : zipfile\n",
      "INFO:Details:Import : IPython\n",
      "INFO:Details:Import : tensorflow\n",
      "INFO:Phases:Project Specific Package Load : End\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Importing Project Specific Packages\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Importing Project Specific Packages : Start')\n",
    "\n",
    "# Identifying Customer Targets (Python)\n",
    "details_logger.info('Import : inline')\n",
    "%matplotlib inline\n",
    "\n",
    "# import packages for text processing and machine learning\n",
    "details_logger.info('Import : pandas')\n",
    "import pandas as pd  # DataFrame structure and operations\n",
    "from pandas.plotting import scatter_matrix  # scatter plot matrix\n",
    "\n",
    "details_logger.info('Import : numpy')\n",
    "import numpy as np  # arrays and numerical processing\n",
    "import matplotlib.pyplot as plt  # 2D plotting\n",
    "\n",
    "details_logger.info('Import : scipy')\n",
    "from scipy.stats import uniform  # for training-and-test split\n",
    "\n",
    "details_logger.info('Import : patsy')\n",
    "import patsy  # translate model specification into design matrices\n",
    "\n",
    "details_logger.info('Import : seaborn')\n",
    "import seaborn as sns  # PROVIDES TRELLIS AND SMALL MULTIPLE PLOTTING\n",
    "\n",
    "# import user-defined module\n",
    "# details_logger.info('import evaluate_classifier')\n",
    "# import evaluate_classifier as eval\n",
    "\n",
    "# FOLLOWING PACKAGE BEST IMPORTED AND INSTALLED VIA CONDA PROMPT\n",
    "# conda install -c conda-forge mlxtend\n",
    "\n",
    "# Association Rules Mining\n",
    "# details_logger.info('import mlxtend')\n",
    "# from mlxtend.frequent_patterns import apriori            # EASY ASSOCIATION RULES PACKAGE FROM RABST\n",
    "# from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "details_logger.info('Import : queue')\n",
    "from queue import Queue\n",
    "\n",
    "details_logger.info('Import : threading')\n",
    "import threading\n",
    "\n",
    "details_logger.info('Import : time')\n",
    "import time\n",
    "\n",
    "details_logger.info('Import : sklearn')\n",
    "from sklearn.tree import DecisionTreeRegressor  # machine learning tree\n",
    "from sklearn.ensemble import RandomForestRegressor # ensemble method\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "details_logger.info('Import : statsmodels')\n",
    "import statsmodels.api as sm  # logistic regression\n",
    "import statsmodels.formula.api as smf  # R-like model specification\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "details_logger.info('Import : pmdarima')\n",
    "import pmdarima as pm\n",
    "\n",
    "details_logger.info('Import : zipfile')\n",
    "import zipfile\n",
    "\n",
    "details_logger.info('Import : IPython')\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "details_logger.info('Import : tensorflow')\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "\n",
    "phases_logger.info('Project Specific Package Load : End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Details:PWD: /Users/cbgarrett/Documents/ist718project/devel003/predictingstockprice\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Present Working Directory\n",
    "# ==============================================================================\n",
    "\n",
    "# This will print the current directory : Debugging purposes\n",
    "details_logger.info('PWD: ' + os.getcwd())\n",
    "PWD = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Information\n",
    "# ==============================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Following should be your tree such that the link 'stock_market_data' points to\n",
    "# actual directory 'stock_market_data' that is 2 directories above\n",
    "\n",
    "# \n",
    "# MY_PROJECT_DIR\n",
    "# ├── BITBUCKET_CHECKOUT_DIR_000\n",
    "# │   └── predictingstockprice\n",
    "# │       ├── OtherTSVs\n",
    "# │       ├── lib\n",
    "# │       ├── stock_market_data -> ../../stock_market_data\n",
    "# │       └── CSV -> ../../CSV\n",
    "# ├── stock_market_data\n",
    "# │   ├── forbes2000\n",
    "# │   │   ├── csv\n",
    "# │   │   └── json\n",
    "# │   ├── nasdaq\n",
    "# │   │   ├── csv\n",
    "# │   │   └── json\n",
    "# │   ├── nyse\n",
    "# │   │   ├── csv\n",
    "# │   │   └── json\n",
    "# │   └── sp500\n",
    "# │       ├── csv\n",
    "# │       └── json\n",
    "# └── CSV\n",
    "#     ├── AdjustedClose_df.csv.zip\n",
    "#     ├── Close_df.csv.zip\n",
    "#     ├── High_df.csv.zip\n",
    "#     ├── Low_df.csv.zip\n",
    "#     └── Open_df.csv.zip\n",
    "# \n",
    "\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Phases:User Settings : Start\n",
      "INFO:Details:User Settings:\n",
      "INFO:Details:User Setting : stock_market_data_path = /Users/cbgarrett/Documents/ist718project/stock_market_data\n",
      "INFO:Details:User Setting : USE_Open_df = True\n",
      "INFO:Details:User Setting : USE_Close_df = True\n",
      "INFO:Details:User Setting : USE_Low_df = True\n",
      "INFO:Details:User Setting : USE_High_df = True\n",
      "INFO:Details:User Setting : USE_AdjustedClose_df = True\n",
      "INFO:Details:User Setting : breakAtIteration = 0 : Do not break\n",
      "INFO:Details:User Setting : Number of models to be run = 410\n",
      "INFO:Details:User Setting : write_CSV = False\n",
      "INFO:Details:User Setting : write_TSV = False\n",
      "INFO:Details:User Setting : Last sampled date = 24/10/2022\n",
      "INFO:Details:User Settings : Number of models  = 410\n",
      "INFO:Details:User Settings : Maximum Threads   = 2\n",
      "INFO:Details:User Settings : Forecast Samples  = 260\n",
      "INFO:Phases:User Settings : End\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# User Settings\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('User Settings : Start')\n",
    "\n",
    "# Logging User Settings:\n",
    "#\n",
    "details_logger.info('User Settings:')\n",
    "\n",
    "\n",
    "## Uncomment following to override the stock_market_data_path\n",
    "\n",
    "# USER_SETTING_HERE\n",
    "# \n",
    "# If the tree structure thing does not work for you, you may have to use the following paths instead:\n",
    "## Ayush:\n",
    "# stock_market_data_path = 'Add_your_path_here'\n",
    "## CB:\n",
    "# stock_market_data_path = '/Users/cbgarrett/Documents/ist718project/stock_market_data'\n",
    "## Richard:\n",
    "# stock_market_data_path = 'Add_your_path_here'\n",
    "## Niranjan:\n",
    "# stock_market_data_path = '/Users/niranjanjuvekar/MyStuff/Education_Niranjan/Data_Science_Syracuse_University/12_IST_718/Project/stock_market_data'\n",
    "\n",
    "\n",
    "\n",
    "# USER_SETTING_HERE\n",
    "# Select the dataFrame(s) to be used\n",
    "# \n",
    "USE_Open_df          = True\n",
    "USE_Close_df         = True\n",
    "USE_Low_df           = True\n",
    "USE_High_df          = True\n",
    "USE_Volume_df        = False\n",
    "USE_AdjustedClose_df = True\n",
    "\n",
    "\n",
    "# USER_SETTING_HERE\n",
    "# Variable breakAtIteration to be used for debugging purposes\n",
    "# if breakAtIteration = 0 : goes through all files\n",
    "#\n",
    "breakAtIteration = 0\n",
    "\n",
    "    \n",
    "# USER_SETTING_HERE\n",
    "# Once the dataFrame has been processed, we can\n",
    "# write it to the disk in TSV or CSV format\n",
    "#\n",
    "write_CSV = False\n",
    "write_TSV = False\n",
    "\n",
    "\n",
    "# USER_SETTINGS_HERE\n",
    "# Maximum number of parallel threads to be spawned\n",
    "#\n",
    "maxThreads = 2\n",
    "#maxThreads = 6\n",
    "\n",
    "\n",
    "# USER_SETTING_HERE\n",
    "# Number of models to be run\n",
    "#\n",
    "N = 410\n",
    "# N = 400\n",
    "# N = 200\n",
    "# N = 100\n",
    "# N = 50\n",
    "#N = 6 # CB : Comment out this line. This is just for demo.\n",
    "# N = 1 # Run only one model (Default)\n",
    "\n",
    "\n",
    "# USER_SETTINGS_HERE\n",
    "# Number of forecasting periods (Unit : Business days)\n",
    "#\n",
    "n_periods = 260\n",
    "\n",
    "\n",
    "# USER_SETTING_HERE\n",
    "# Last date with sampled values beyond which the dataFrame / CSV would be empty\n",
    "LastSampledDate = '24/10/2022'\n",
    "\n",
    "\n",
    "# USER_SETTING_HERE\n",
    "# Output Dir\n",
    "outputDir = 'Output'\n",
    "\n",
    "\n",
    "# USER_SETTING_HERE\n",
    "# Number of EPOCHs for the LSTM Model\n",
    "LSTM_EPOCHS = 25\n",
    "#LSTM_EPOCHS = 2 # CB : Remove this line. This is just for demo.\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Processing and Logging the User Settings\n",
    "# \n",
    "\n",
    "\n",
    "# Using stock_market_data link in the current directory\n",
    "# stock_market_data -> ../../stock_market_data\n",
    "#\n",
    "stock_market_data_path = os.path.realpath('stock_market_data')\n",
    "details_logger.info('User Setting : stock_market_data_path = ' + stock_market_data_path)\n",
    "\n",
    "\n",
    "dfFileList = []\n",
    "\n",
    "if USE_Open_df:\n",
    "    details_logger.info('User Setting : USE_Open_df = True')\n",
    "    dfFileList.append('Open')\n",
    "if USE_Close_df:\n",
    "    details_logger.info('User Setting : USE_Close_df = True')\n",
    "    dfFileList.append('Close')\n",
    "if USE_Low_df:\n",
    "    details_logger.info('User Setting : USE_Low_df = True')\n",
    "    dfFileList.append('Low')\n",
    "if USE_High_df:\n",
    "    details_logger.info('User Setting : USE_High_df = True')\n",
    "    dfFileList.append('High')\n",
    "if USE_Volume_df:\n",
    "    details_logger.info('User Setting : USE_Volume_df = True')\n",
    "    dfFileList.append('Volume')\n",
    "if USE_AdjustedClose_df:\n",
    "    details_logger.info('User Setting : USE_AdjustedClose_df = True')\n",
    "    dfFileList.append('AdjustedClose')\n",
    "if len(dfFileList) == 0:\n",
    "    details_logger.info('Please select at least one dataFrame to operate on')\n",
    "    details_logger.info('Set one of these to True in the user settings:')\n",
    "    details_logger.info('USE_Low_df / USE_Open_df / USE_Volume_df / USE_High_df / USE_Close_df / USE_AdjustedClose_df')\n",
    "    print('Dataframe(s) not specified. See details log for the error')\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "if breakAtIteration == 0:\n",
    "    details_logger.info('User Setting : breakAtIteration = 0 : Do not break')\n",
    "else:\n",
    "    details_logger.info('User Setting : breakAtIteration = ' + str(breakAtIteration))\n",
    "\n",
    "\n",
    "details_logger.info('User Setting : Number of models to be run = ' + str(N))\n",
    "    \n",
    "    \n",
    "details_logger.info('User Setting : write_CSV = ' + str(write_CSV))\n",
    "details_logger.info('User Setting : write_TSV = ' + str(write_TSV))\n",
    "\n",
    "\n",
    "details_logger.info('User Setting : Last sampled date = ' + LastSampledDate)\n",
    "\n",
    "\n",
    "\n",
    "details_logger.info('User Settings : Number of models  = ' + str(N))\n",
    "details_logger.info('User Settings : Maximum Threads   = ' + str(maxThreads))\n",
    "details_logger.info('User Settings : Forecast Samples  = ' + str(n_periods))\n",
    "\n",
    "\n",
    "\n",
    "phases_logger.info('User Settings : End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Unzip required dataFrame files\n",
    "# ==============================================================================\n",
    "\n",
    "for file in dfFileList:\n",
    "    with zipfile.ZipFile(PWD+'/CSV/'+file+'_df.csv.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(PWD+'/CSV/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Reading the dataFrames\n",
    "# ==============================================================================\n",
    "\n",
    "if 'Open' in dfFileList:\n",
    "    OpenDF_tickerSymbols = pd.read_csv(PWD+'/CSV/Open_df.csv', sep=',', header='infer')\n",
    "    OpenDF_tickerSymbols['Date'] = OpenDF_tickerSymbols['Date'].str.replace('-','/')\n",
    "    # pd.to_datetime(OpenDF_tickerSymbols['Date'], infer_datetime_format=True)\n",
    "    OpenDF_tickerSymbols = OpenDF_tickerSymbols.set_index('Date')\n",
    "    OpenDF_tickerSymbols = OpenDF_tickerSymbols.loc[:LastSampledDate]\n",
    "    OpenDF_tickerSymbols = OpenDF_tickerSymbols.fillna(method='ffill')\n",
    "\n",
    "if 'Close' in dfFileList:\n",
    "    CloseDF_tickerSymbols = pd.read_csv(PWD+'/CSV/Close_df.csv', sep=',', header='infer')\n",
    "    CloseDF_tickerSymbols['Date'] = CloseDF_tickerSymbols['Date'].str.replace('-','/')\n",
    "    # pd.to_datetime(CloseDF_tickerSymbols['Date'], infer_datetime_format=True)\n",
    "    CloseDF_tickerSymbols = CloseDF_tickerSymbols.set_index('Date')\n",
    "    CloseDF_tickerSymbols = CloseDF_tickerSymbols.loc[:LastSampledDate]\n",
    "    CloseDF_tickerSymbols = CloseDF_tickerSymbols.fillna(method='ffill')\n",
    "\n",
    "if 'Low' in dfFileList:\n",
    "    LowDF_tickerSymbols = pd.read_csv(PWD+'/CSV/Low_df.csv', sep=',', header='infer')\n",
    "    LowDF_tickerSymbols['Date'] = LowDF_tickerSymbols['Date'].str.replace('-','/')\n",
    "    # pd.to_datetime(LowDF_tickerSymbols['Date'], infer_datetime_format=True)\n",
    "    LowDF_tickerSymbols = LowDF_tickerSymbols.set_index('Date')\n",
    "    LowDF_tickerSymbols = LowDF_tickerSymbols.loc[:LastSampledDate]\n",
    "    LowDF_tickerSymbols = LowDF_tickerSymbols.fillna(method='ffill')\n",
    "    \n",
    "if 'High' in dfFileList:\n",
    "    HighDF_tickerSymbols = pd.read_csv(PWD+'/CSV/High_df.csv', sep=',', header='infer')\n",
    "    HighDF_tickerSymbols['Date'] = HighDF_tickerSymbols['Date'].str.replace('-','/')\n",
    "    # pd.to_datetime(HighDF_tickerSymbols['Date'], infer_datetime_format=True)\n",
    "    HighDF_tickerSymbols = HighDF_tickerSymbols.set_index('Date')\n",
    "    HighDF_tickerSymbols = HighDF_tickerSymbols.loc[:LastSampledDate]\n",
    "    HighDF_tickerSymbols = HighDF_tickerSymbols.fillna(method='ffill')\n",
    "\n",
    "if 'Volume' in dfFileList:\n",
    "    VolumeDF_tickerSymbols = pd.read_csv(PWD+'/CSV/Volume_df.csv', sep=',', header='infer')\n",
    "    VolumeDF_tickerSymbols['Date'] = VolumeDF_tickerSymbols['Date'].str.replace('-','/')\n",
    "    # pd.to_datetime(VolumeDF_tickerSymbols['Date'], infer_datetime_format=True)\n",
    "    VolumeDF_tickerSymbols = VolumeDF_tickerSymbols.set_index('Date')\n",
    "    VolumeDF_tickerSymbols = VolumeDF_tickerSymbols.loc[:LastSampledDate]\n",
    "    VolumeDF_tickerSymbols = VolumeDF_tickerSymbols.fillna(method='ffill')\n",
    "\n",
    "if 'AdjustedClose' in dfFileList:\n",
    "    AdjustedCloseDF_tickerSymbols = pd.read_csv(PWD+'/CSV/AdjustedClose_df.csv', sep=',', header='infer')\n",
    "    AdjustedCloseDF_tickerSymbols['Date'] = AdjustedCloseDF_tickerSymbols['Date'].str.replace('-','/')\n",
    "    # pd.to_datetime(AdjustedCloseDF_tickerSymbols['Date'], infer_datetime_format=True)\n",
    "    AdjustedCloseDF_tickerSymbols = AdjustedCloseDF_tickerSymbols.set_index('Date')\n",
    "    AdjustedCloseDF_tickerSymbols = AdjustedCloseDF_tickerSymbols.loc[:LastSampledDate]\n",
    "    AdjustedCloseDF_tickerSymbols = AdjustedCloseDF_tickerSymbols.fillna(method='ffill')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenDF_tickerSymbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Example:\\nmydf = dropNan(MYDF[['CSCO', 'SBUX']])\\nprint(mydf)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# dropNan function to drop empty rows from given dataFrame\n",
    "# ==============================================================================\n",
    "\n",
    "def dropNan (df):\n",
    "    ignoreColumns = ['Unnamed: 0', 'Date', 'WeekDay']\n",
    "    columns = df.columns\n",
    "    # print(columns)\n",
    "    for i in columns:\n",
    "        if i not in ignoreColumns:\n",
    "            # print('i = ', i)\n",
    "            df = df[df[i].notna()]\n",
    "    # print(df)\n",
    "    return(df)\n",
    "\n",
    "'''\n",
    "# Example:\n",
    "mydf = dropNan(MYDF[['CSCO', 'SBUX']])\n",
    "print(mydf)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Phases:Initialize for Models Generation : Start\n",
      "INFO:Phases:Initialize for Models Generation : End\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Initialize variables for model generation\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Initialize for Models Generation : Start')\n",
    "\n",
    "# Threads Dictionary\n",
    "t = {}\n",
    "\n",
    "# Queue Settings\n",
    "q = Queue(maxsize = maxThreads)\n",
    "\n",
    "# Model Dictionary\n",
    "model = {}\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "\n",
    "\n",
    "# Model Generation Tracker : Keeps track of what models are built and being built\n",
    "modelGenTracker = []\n",
    "modelGenPointer = len(modelGenTracker) - 1\n",
    "\n",
    "\n",
    "phases_logger.info('Initialize for Models Generation : End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Phases:Initialize Dictionaries : Start\n",
      "INFO:Phases:Initialize Dictionaries : End\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Model-Specific : Initialize Dictionaries\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Initialize Dictionaries : Start')\n",
    "\n",
    "LSTM_models_dict = {}\n",
    "LSTM_forecast_dict = {}\n",
    "LSTM_model_residuals_dict = {}\n",
    "\n",
    "phases_logger.info('Initialize Dictionaries : End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Phases:Create Model Definition : Start\n",
      "INFO:Phases:Create Model Definition : End\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Model-Specific : Create Model Definition : createModel_AutoARIMA\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Create Model Definition : Start')\n",
    "\n",
    "def createModel_LSTM(modelNumber):\n",
    "    \n",
    "    tickerSymbol = MYDF.columns[modelNumber]\n",
    "    print('Model number : ', modelNumber, ' :: Ticker Symbol : ', tickerSymbol)\n",
    "    details_logger.info('Model number : ' + str(modelNumber) + ' :: Ticker Symbol : ' + tickerSymbol)\n",
    "\n",
    "    mydf = MYDF[tickerSymbol].to_frame()\n",
    "    mydf.replace(\"\", nan_value, inplace=True)\n",
    "    mydf.dropna(inplace=True)\n",
    "\n",
    "    data = mydf[tickerSymbol].values\n",
    "    data = data.reshape((-1,1))\n",
    "\n",
    "    split_percent = 0.80\n",
    "    split = int(split_percent*len(data))\n",
    "\n",
    "    X_train = data[:split]\n",
    "    X_test  = data[split:]\n",
    "\n",
    "    date_train = mydf.index.values[:split]\n",
    "    date_test = mydf.index.values[split:]\n",
    "\n",
    "\n",
    "    look_back = 15\n",
    "\n",
    "    train_generator = TimeseriesGenerator(X_train, X_train, length=look_back, batch_size=20)     \n",
    "    test_generator = TimeseriesGenerator(X_test, X_test, length=look_back, batch_size=1)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(10,\n",
    "            activation='relu',\n",
    "            input_shape=(look_back,1))\n",
    "    )\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    num_epochs = LSTM_EPOCHS\n",
    "    model.fit_generator(train_generator, epochs=num_epochs, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "    data = data.reshape((-1))\n",
    "\n",
    "    def predict(num_prediction, model):\n",
    "        prediction_list = data[-look_back:]\n",
    "\n",
    "        for _ in range(num_prediction):\n",
    "            x = prediction_list[-look_back:]\n",
    "            x = x.reshape((1, look_back, 1))\n",
    "            out = model.predict(x)[0][0]\n",
    "            prediction_list = np.append(prediction_list, out)\n",
    "        prediction_list = prediction_list[look_back-1:]\n",
    "\n",
    "        return prediction_list\n",
    "\n",
    "    def predict_dates(num_prediction):\n",
    "        last_date = mydf.index.values[-1]\n",
    "        prediction_dates = pd.date_range(last_date, periods=num_prediction+1).tolist()\n",
    "        return prediction_dates\n",
    "\n",
    "    num_prediction = 30\n",
    "    forecast = predict(num_prediction, model)\n",
    "    forecast_dates = predict_dates(num_prediction)\n",
    "    LSTM_forecast_dict[tickerSymbol] = forecast\n",
    "\n",
    "\n",
    "    q.get()\n",
    "    details_logger.info('Thread Spawn : ' + str(modelNumber) + ' : Done')\n",
    "\n",
    "    modelGenTracker.append(modelNumber)\n",
    "\n",
    "    \n",
    "phases_logger.info('Create Model Definition : End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Phases:Making Models in Threads : End\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Running Threads : Making Models in Threads\n",
    "# ==============================================================================\n",
    "\n",
    "phases_logger.info('Making Models in Threads : Start')\n",
    "details_logger.info('Making Models in Threads')\n",
    "\n",
    "for file in dfFileList:\n",
    "\n",
    "    details_logger.info('Running Threads : File : ' + file + ' : Start')\n",
    "    modelGenTracker = []\n",
    "    modelGenPointer = len(modelGenTracker) - 1\n",
    "    \n",
    "    if file == 'Open':\n",
    "        MYDF = OpenDF_tickerSymbols.iloc[:, 2:]\n",
    "    if file == 'Close':\n",
    "        MYDF = CloseDF_tickerSymbols.iloc[:, 2:]\n",
    "    if file == 'Low':\n",
    "        MYDF = LowDF_tickerSymbols.iloc[:, 2:]\n",
    "    if file == 'High':\n",
    "        MYDF = HighDF_tickerSymbols.iloc[:, 2:]\n",
    "    if file == 'Volume':\n",
    "        MYDF = VolumeDF_tickerSymbols.iloc[:, 2:]\n",
    "    if file == 'AdjustedClose':\n",
    "        MYDF = AdjustedCloseDF_tickerSymbols.iloc[:, 2:]\n",
    "\n",
    "    for modelNumber in range(0,N,1):\n",
    "        details_logger.info('Thread Spawn : ' + str(modelNumber) + ' : Starting')\n",
    "        while(q.full()):\n",
    "            # print('Sleeping 1 sec')\n",
    "            details_logger.info('Sleeping 1 sec')\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "        if modelGenPointer < len(modelGenTracker) - 1:\n",
    "            for i in range (modelGenPointer, len(modelGenTracker), 1):\n",
    "                details_logger.info('Thread Join : ' + str(modelGenTracker[i]))\n",
    "                t[modelGenTracker[i]].join()\n",
    "                clear_output(wait=False)\n",
    "            modelGenPointer = len(modelGenTracker) - 1\n",
    "\n",
    "\n",
    "        # Creating Threads\n",
    "        details_logger.info('t[modelNumber] = ' + str(modelNumber))\n",
    "        t[modelNumber] = threading.Thread(target=createModel_LSTM, args=(modelNumber,))\n",
    "\n",
    "\n",
    "        # Running the thread\n",
    "        details_logger.info('Q : put' + str(modelNumber))\n",
    "        q.put(modelNumber)\n",
    "        t[modelNumber].start()\n",
    "\n",
    "#     time.sleep(100) TESTING THIS \n",
    "\n",
    "    while (len(modelGenTracker) < N):\n",
    "        if modelGenPointer < len(modelGenTracker) - 1:\n",
    "            for i in range (modelGenPointer, len(modelGenTracker), 1):\n",
    "                details_logger.info('Thread Join : ' + str(modelGenTracker[i]))\n",
    "                t[modelGenTracker[i]].join()\n",
    "                clear_output(wait=False)\n",
    "            modelGenPointer = len(modelGenTracker) - 1\n",
    "        \n",
    "        # For debugging:\n",
    "        # print('modelGenTracker : ', modelGenTracker) # HERE\n",
    "        # print('modelGenPointer : ', modelGenPointer) # HERE\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    LSTM_forecast_df = pd.DataFrame.from_dict(LSTM_forecast_dict, orient='index')\n",
    "    LSTM_forecast_df.to_csv(outputDir+'/'+'LSTM_forecast_'+file+'df.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "    details_logger.info('Running Threads : File : ' + file + ' : End')\n",
    "    \n",
    "clear_output(wait=False)\n",
    "\n",
    "phases_logger.info('Making Models in Threads : End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
